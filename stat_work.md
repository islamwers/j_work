# Часть 1. Условие задачи и данные

Имеется выборка пар $\{(x_i,y_i)\}_{i=1}^{n}$, где $x_i$ — независимая переменная, $y_i$ — зависимая, $n=12$:

| $i$ | $x_i$  | $y_i$   |
|:---:|:------:|:--------|
| 1   | -0.502 | -0.900  |
| 2   |  1.803 |  3.124  |
| 3   |  0.928 |  0.455  |
| 4   |  0.395 | -0.342  |
| 5   | -1.376 |  0.158  |
| 6   | -1.376 |  0.159  |
| 7   | -1.768 |  1.557  |
| 8   |  1.465 |  2.031  |
| 9   |  0.404 | -0.728  |
| 10  |  0.832 |  0.217  |
| 11  | -1.918 |  1.626  |
| 12  |  1.880 |  3.380  |

Предполагаем полиномиальную модель порядка $p$:

$$
y_i = a_0 + \sum_{k=1}^{p} a_k x_i^{k} + \varepsilon_i,
$$

где $\{\varepsilon_i\}$ — независимые одинаково распределённые случайные величины,

$$
\varepsilon_i \sim N(0,\sigma^2).
$$

Нужно найти оценки $\hat a_k$ методом наименьших квадратов для $p=0,1,2$ и для каждой модели
проверить гипотезу о нормальности остатков

$$
\hat\varepsilon_i = y_i - \hat a_0 - \sum_{k=1}^{p} \hat a_k x_i^{k}.
$$

---

# 2. Общая формулировка МНК в матричном виде

Введём матрицу регрессоров $X$ и вектор коэффициентов $a$.

Для полинома порядка $p$:

$$
X =
\begin{pmatrix}
1 & x_1 & x_1^2 & \dots & x_1^p \\
1 & x_2 & x_2^2 & \dots & x_2^p \\
\vdots & \vdots & \vdots & & \vdots \\
1 & x_n & x_n^2 & \dots & x_n^p
\end{pmatrix},
\qquad
a =
\begin{pmatrix}
a_0 \\ a_1 \\ \vdots \\ a_p
\end{pmatrix},
\qquad
y =
\begin{pmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix}.
$$

Модель:

$$
y = Xa + \varepsilon.
$$

Оценка МНК:

$$
\hat a = (X^\top X)^{-1} X^\top y.
$$

Остатки:

$$
\hat\varepsilon = y - X\hat a.
$$

---

# 3. Модель порядка 0 (константа)

Модель:

$$
y_i = a_0 + \varepsilon_i.
$$

Тогда $X$ — столбец из единиц, а оценка МНК совпадает с выборочным средним:

$$
\hat a_0 = \bar y = \frac{1}{n}\sum_{i=1}^{n} y_i.
$$

Посчитаем:

$$
\bar y \approx 0.89475.
$$

Итак, модель:

$$
\hat y_i^{(0)} = \hat a_0 \approx 0.89475.
$$

Остатки:

$$
\hat\varepsilon_i^{(0)} = y_i - \hat a_0.
$$

(Численные значения с округлением до $3$ знаков.)

| $i$ | $y_i$ | $\hat y_i^{(0)}$ | $\hat\varepsilon_i^{(0)}$ |
|:---:|:-----:|:----------------:|:-------------------------:|
| 1   | -0.900 | 0.895 | -1.795 |
| 2   |  3.124 | 0.895 |  2.229 |
| 3   |  0.455 | 0.895 | -0.440 |
| 4   | -0.342 | 0.895 | -1.237 |
| 5   |  0.158 | 0.895 | -0.737 |
| 6   |  0.159 | 0.895 | -0.736 |
| 7   |  1.557 | 0.895 |  0.662 |
| 8   |  2.031 | 0.895 |  1.136 |
| 9   | -0.728 | 0.895 | -1.623 |
| 10  |  0.217 | 0.895 | -0.678 |
| 11  |  1.626 | 0.895 |  0.731 |
| 12  |  3.380 | 0.895 |  2.485 |


### Критерий Шапиро–Уилка для проверки нормальности

Для проверки гипотезы о нормальности остатков используется критерий Шапиро–Уилка:

$$
H_0: \varepsilon_i \sim N(0,\sigma^2), \qquad
H_1: \text{распределение не нормально}.
$$

Статистика критерия имеет вид:

$$
W = 
\frac{
\left( \sum_{i=1}^{n} a_i \, x_{(n-i+1)} - a_i \, x_{(i)} \right)^2
}{
\sum_{i=1}^{n} (x_i - \bar{x})^2
},
$$

где:

- $x_{(1)} \le \dots \le x_{(n)}$ — упорядоченные значения выборки,
- $a_i$ — табличные коэффициенты, зависящие только от $n$,
- $\bar{x}$ — среднее выборки.

Интерпретация результата:

- если $p\text{-value} > \alpha$, то гипотеза $H_0$ **не отвергается** — данные согласуются с нормальным распределением;
- если $p\text{-value} \le \alpha$, то $H_0$ **отвергается** — распределение не является нормальным.


## Проверка нормальности остатков (порядок 0)

Проверяем гипотезу

$$
H_0 : \hat\varepsilon_i^{(0)} \sim N(0,\sigma^2)
$$

с помощью критерия Шапиро–Уилка.

По расчётам:

- статистика $W \approx 0.919$,
- $p\text{-value} \approx 0.277$.

Так как $p > 0.05$, на уровне значимости $\alpha = 0.05$
нет оснований отвергнуть $H_0$ — нормальность остатков для модели порядка 0
не противоречит данным.

---

# 4. Линейная модель ($p=1$)

Модель:

$$
y_i = a_0 + a_1 x_i + \varepsilon_i.
$$

Для простой линейной регрессии оценки МНК можно записать в скалярном виде:

- $\bar x = \dfrac{1}{n} \sum_{i=1}^{n} x_i$,
- $\bar y = \dfrac{1}{n} \sum_{i=1}^{n} y_i$,
- $S_{xx} = \sum_{i=1}^{n} (x_i - \bar x)^2$,
- $S_{xy} = \sum_{i=1}^{n} (x_i - \bar x)(y_i - \bar y)$,

тогда

$$
\hat a_1 = \frac{S_{xy}}{S_{xx}}, \qquad
\hat a_0 = \bar y - \hat a_1 \bar x.
$$

По выборке:

- $\bar x \approx 0.0639$,
- $\bar y \approx 0.8948$,
- $S_{xx} \approx 21.5984$,
- $S_{xy} \approx 8.5939$.

Отсюда

$$
\hat a_1 \approx \frac{8.5939}{21.5984} \approx 0.3979,
$$

$$
\hat a_0 \approx 0.8948 - 0.3979 \cdot 0.0639 \approx 0.8693.
$$

Линейная модель МНК:

$$
\hat y_i^{(1)} = \hat a_0 + \hat a_1 x_i \approx 0.8693 + 0.3979 x_i.
$$

Остатки:

$$
\hat\varepsilon_i^{(1)} = y_i - \hat y_i^{(1)}.
$$

| $i$ | $x_i$  | $y_i$ | $\hat y_i^{(1)}$ | $\hat\varepsilon_i^{(1)}$ |
|:---:|:------:|:-----:|:----------------:|:-------------------------:|
| 1   | -0.502 | -0.900 | 0.670 | -1.570 |
| 2   |  1.803 |  3.124 | 1.587 |  1.537 |
| 3   |  0.928 |  0.455 | 1.239 | -0.784 |
| 4   |  0.395 | -0.342 | 1.026 | -1.368 |
| 5   | -1.376 |  0.158 | 0.322 | -0.164 |
| 6   | -1.376 |  0.159 | 0.322 | -0.163 |
| 7   | -1.768 |  1.557 | 0.166 |  1.391 |
| 8   |  1.465 |  2.031 | 1.452 |  0.579 |
| 9   |  0.404 | -0.728 | 1.030 | -1.758 |
| 10  |  0.832 |  0.217 | 1.200 | -0.983 |
| 11  | -1.918 |  1.626 | 0.106 |  1.520 |
| 12  |  1.880 |  3.380 | 1.617 |  1.763 |

## Проверка нормальности остатков (порядок 1)

Проверяем

$$
H_0 : \hat\varepsilon_i^{(1)} \sim N(0,\sigma^2)
$$

критерием Шапиро–Уилка.

Результаты:

- $W \approx 0.897$,
- $p\text{-value} \approx 0.147$.

Так как $p > 0.05$, гипотеза о нормальности остатков **не отвергается**.

---

# 5. Квадратичная модель ($p=2$)

Модель:

$$
y_i = a_0 + a_1 x_i + a_2 x_i^2 + \varepsilon_i.
$$

Матрица регрессоров:

$$
X =
\begin{pmatrix}
1 & x_1 & x_1^2 \\
1 & x_2 & x_2^2 \\
\vdots & \vdots & \vdots \\
1 & x_n & x_n^2
\end{pmatrix}.
$$

Оценка МНК:

$$
\hat a = (X^\top X)^{-1} X^\top y.
$$

Численные результаты:

$$
\hat a_0 \approx -0.8809, \qquad
\hat a_1 \approx 0.4941, \qquad
\hat a_2 \approx 0.9668.
$$

Квадратичная модель:

$$
\hat y_i^{(2)} = -0.8809 + 0.4941 x_i + 0.9668 x_i^2.
$$

Остатки:

$$
\hat\varepsilon_i^{(2)} = y_i - \hat y_i^{(2)}.
$$

| $i$ | $x_i$  | $y_i$ | $\hat y_i^{(2)}$ | $\hat\varepsilon_i^{(2)}$ |
|:---:|:------:|:-----:|:----------------:|:-------------------------:|
| 1   | -0.502 | -0.900 | -0.885 | -0.015 |
| 2   |  1.803 |  3.124 |  3.153 | -0.029 |
| 3   |  0.928 |  0.455 |  0.410 |  0.045 |
| 4   |  0.395 | -0.342 | -0.535 |  0.193 |
| 5   | -1.376 |  0.158 |  0.270 | -0.112 |
| 6   | -1.376 |  0.159 |  0.270 | -0.111 |
| 7   | -1.768 |  1.557 |  1.268 |  0.289 |
| 8   |  1.465 |  2.031 |  1.918 |  0.113 |
| 9   |  0.404 | -0.728 | -0.523 | -0.205 |
| 10  |  0.832 |  0.217 |  0.199 |  0.018 |
| 11  | -1.918 |  1.626 |  1.728 | -0.102 |
| 12  |  1.880 |  3.380 |  3.465 | -0.085 |

Остатки по модулю малы.

## Проверка нормальности остатков (порядок 2)

Проверяем

$$
H_0 : \hat\varepsilon_i^{(2)} \sim N(0,\sigma^2)
$$

критерием Шапиро–Уилка.

Результаты:

- $W \approx 0.945$,
- $p\text{-value} \approx 0.570$.

Так как $p \gg 0.05$, гипотеза о нормальном распределении остатков **не отвергается**.

---

# 6. Краткое сравнение моделей

Для наглядности рассмотрим сумму квадратов остатков (SSE):

- порядок $0$: $\text{SSE}_0 \approx 22.53$;
- порядок $1$: $\text{SSE}_1 \approx 19.11$;
- порядок $2$: $\text{SSE}_2 \approx 0.22$.

Коэффициент детерминации:

$$
R^2 = 1 - \frac{\text{SSE}}{\text{SST}},
$$

где $\text{SST}$ — сумма квадратов отклонений $y_i$ от $\bar y$.

- $R_0^2 = 0$ (тривиальная модель);
- $R_1^2 \approx 0.152$;
- $R_2^2 \approx 0.990$.

---

# 7. Выводы по первой части

1. Получены оценки параметров:

   - Полином порядка $0$:

$$
\hat y = 0.8948.
$$

   - Полином порядка $1$:

$$
\hat y = 0.8693 + 0.3979 x.
$$

   - Полином порядка $2$:

$$
     \hat y = -0.8809 + 0.4941 x + 0.9668 x^2.
$$



3. Для всех трёх моделей проверка нормальности остатков критерием Шапиро–Уилка показала $p\text{-value} > 0.05$, т.е. на уровне значимости $0.05$ во всех случаях гипотеза о нормальности остатков **не отвергается**.

4. Квадратичная модель ($p=2$) значительно лучше описывает данные (очень малые остатки и $R^2 \approx 0.99$), при этом её остатки также не противоречат нормальному распределению.


# Часть 2. Численное исследование поведения полиномиальной регрессии

В этой части исследуем поведение оценок коэффициентов полиномиальной регрессии при:

1. добавлении нового признака вида  
   $z_i = x_i + \xi_i$, где шум $\xi_i \sim N(0,\sigma_\xi^2)$,  
   при различных значениях дисперсии $\sigma_\xi^2$;

2. добавлении в выборку **одного «далёкого» наблюдения** $(k,k)$ при $k \to \infty$  
   для исходной квадратичной модели.

Исходные данные $(x_i, y_i)$ — те же, что использовались в части 1, размер выборки $n = 12$.

---

## 2.1. Модель с дополнительным шумовым признаком

Рассматриваем расширенную модель:

$$
y_i = a_0 + a_1 x_i + a_2 x_i^2 + a_3 z_i + \varepsilon_i, \quad
z_i = x_i + \xi_i,\quad \xi_i \sim N(0,\sigma_\xi^2).
$$

Цель: проследить, как зависят оценки коэффициентов
$\hat a_0, \hat a_1, \hat a_2, \hat a_3$ от дисперсии шума $\sigma_\xi^2$.

Для этого:

1. выбираем набор значений $\sigma_\xi^2$ от очень маленьких до очень больших;
2. для каждого значения дисперсии многократно (по $n_{\text{sim}}$ раз) генерируем шум $\xi_i$,
   строим признак $z_i = x_i + \xi_i$, оцениваем модель МНК;
3. усредняем полученные оценки коэффициентов по всем симуляциям;
4. выводим средние значения и строим график зависимости коэффициентов от $\sigma_\xi^2$.

---

## 2.2. Модель с добавлением одной точки $(k,k)$

Рассматриваем исходную квадратичную модель без дополнительного признака:

$$
y_i = a_0 + a_1 x_i + a_2 x_i^2 + \varepsilon_i.
$$

К исходной выборке из 12 наблюдений добавляем одно наблюдение

$$
(x_{n+1}, y_{n+1}) = (k, k),
$$

где $k$ последовательно принимает большие значения (например, $k = 2, 5, 10, \dots, 1000$).

Для каждого $k$:

1. достраиваем выборку до $n+1$ наблюдений;
2. переоцениваем коэффициенты квадратичной регрессии;
3. анализируем, как зависят $\hat a_0(k), \hat a_1(k), \hat a_2(k)$ от величины $k$;
4. строим графики зависимостей коэффициентов от $k$.

---

## 2.3. Реализация численного эксперимента (Python)

Ниже приведён полный код на Python, реализующий оба эксперимента и строящий необходимые графики.
Код можно выполнить в Jupyter Notebook, Google Colab или в обычном `.py`-файле
(в последнем случае графики откроются в отдельном окне).

```python
import numpy as np

# Для воспроизводимости
np.random.seed(42)

# -----------------------------
# 0. Исходные данные
# -----------------------------
x = np.array([
    -0.502,  1.803,  0.928,  0.395,
    -1.376, -1.376, -1.768,  1.465,
     0.404,  0.832, -1.918,  1.880
])
y = np.array([
    -0.900,  3.124,  0.455, -0.342,
     0.158,  0.159,  1.557,  2.031,
    -0.728,  0.217,  1.626,  3.380
])
n = len(x)

# Удобная функция для МНК (без регуляризации)
def ols_coeffs(X, y):
    """
    Возвращает вектор оценок МНК:
    a_hat = (X^T X)^(-1) X^T y
    (на практике используется np.linalg.lstsq,
    которая численно более устойчива).
    """
    a_hat, *_ = np.linalg.lstsq(X, y, rcond=None)
    return a_hat


# ============================================================
# ЧАСТЬ 2.1. Добавление признака z_i = x_i + xi, xi ~ N(0, sigma_xi^2)
# ============================================================

# Будем смотреть на диапазон значений sigma_xi^2
# от очень маленьких до очень больших (логарифмическая сетка)
sigma2_list = np.log10(np.array([1e-4, 1e-3, 1e-2, 1e-1, 1,
                                 1e1, 1e2, 1e3, 1e4]))  # логарифмы дисперсий
sigma2_list = 10 ** sigma2_list   # сами дисперсии

n_sim = 200  # количество повторов для усреднения

# Здесь будем хранить усреднённые оценки коэффициентов [a0, a1, a2, a3]
coefs_vs_sigma = []

for sigma2 in sigma2_list:
    sigma = np.sqrt(sigma2)
    coefs_all = []

    for _ in range(n_sim):
        # генерируем шум xi ~ N(0, sigma^2)
        xi = np.random.normal(loc=0.0, scale=sigma, size=n)
        z = x + xi

        # Матрица регрессоров для модели:
        # y_i = a0 + a1 * x_i + a2 * x_i^2 + a3 * z_i + eps_i
        X_new = np.column_stack([
            np.ones(n),   # столбец единиц
            x,
            x ** 2,
            z
        ])

        a_hat = ols_coeffs(X_new, y)
        coefs_all.append(a_hat)

    coefs_all = np.vstack(coefs_all)
    mean_coefs = coefs_all.mean(axis=0)  # средние значения коэффициентов
    std_coefs = coefs_all.std(axis=0)    # их стандартные отклонения (для информации)

    coefs_vs_sigma.append((sigma2, mean_coefs, std_coefs))

# Печатаем результаты для анализа в табличной форме
print("=== Влияние sigma_xi^2 на коэффициенты модели с признаком z = x + xi ===\n")
print("sigma_xi^2\t a0(mean)\t a1(mean)\t a2(mean)\t a3(mean)")
for sigma2, mean_coefs, std_coefs in coefs_vs_sigma:
    a0, a1, a2, a3 = mean_coefs
    print(f"{sigma2:10.4e}\t {a0: .4f}\t {a1: .4f}\t {a2: .4f}\t {a3: .4f}")

print("\n(Для более детального анализа можно также смотреть std_coefs и строить графики.)")


# ============================================================
# ЧАСТЬ 2.2. Добавление одной точки (k, k) в исходную модель
# ============================================================

# Будем рассматривать квадратичную модель:
# y_i = a0 + a1 * x_i + a2 * x_i^2 + eps_i
# и добавлять точку (k,k) при больших k.

k_values = np.array([2, 5, 10, 20, 50, 100, 200, 500, 1000])

coefs_vs_k = []

for k in k_values:
    x_ext = np.append(x, k)
    y_ext = np.append(y, k)

    X_quad = np.column_stack([
        np.ones(len(x_ext)),
        x_ext,
        x_ext ** 2
    ])

    a_hat = ols_coeffs(X_quad, y_ext)  # [a0, a1, a2]
    coefs_vs_k.append((k, a_hat))

print("\n=== Влияние добавления точки (k,k) на коэффициенты квадратичной регрессии ===\n")
print("k\t a0(k)\t\t a1(k)\t\t a2(k)")
for k, a_hat in coefs_vs_k:
    a0, a1, a2 = a_hat
    print(f"{k:4d}\t {a0: .4f}\t {a1: .4f}\t {a2: .4f}")


# ============================================================
# ПОСТРОЕНИЕ ГРАФИКОВ (если запускаете в Jupyter / Colab)
# ============================================================
import matplotlib.pyplot as plt

# 1) графики a1, a3 в зависимости от sigma_xi^2 (логарифмическая шкала по оси x)
sigma2_vals = np.array([row[0] for row in coefs_vs_sigma])
coefs_mat = np.vstack([row[1] for row in coefs_vs_sigma])  # shape: [len(sigma2), 4]

plt.figure()
plt.xscale("log")
plt.plot(sigma2_vals, coefs_mat[:, 1], marker="o", label="a1 (x)")
plt.plot(sigma2_vals, coefs_mat[:, 3], marker="o", label="a3 (z = x + xi)")
plt.xlabel(r"$\sigma_{\xi}^2$")
plt.ylabel("Оценка коэффициента")
plt.title("Зависимость коэффициентов a1 и a3 от дисперсии шума в новом признаке")
plt.legend()
plt.grid(True)

# 2) графики a0, a1, a2 в зависимости от k
k_vals = np.array([row[0] for row in coefs_vs_k])
coefs_k_mat = np.vstack([row[1] for row in coefs_vs_k])

plt.figure()
plt.plot(k_vals, coefs_k_mat[:, 0], marker="o", label="a0(k)")
plt.plot(k_vals, coefs_k_mat[:, 1], marker="o", label="a1(k)")
plt.plot(k_vals, coefs_k_mat[:, 2], marker="o", label="a2(k)")
plt.xlabel("k")
plt.ylabel("Оценка коэффициента")
plt.title("Изменение коэффициентов при добавлении точки (k,k)")
plt.legend()
plt.grid(True)

plt.show()
```

2.4. Краткая интерпретация полученных результатов

При малых значениях $\sigma_\xi^2$ новый признак $z_i = x_i + \xi_i$ почти линейно зависит от $x_i$, возникает мультколлинеарность. Коэффициенты при $x_i$ и $z_i$ становятся нестабильными и «делят» вклад между собой.

При больших значениях $\sigma_\xi^2$ признак $z_i$ становится практически чистым шумом, некоррелированным с $y_i$. В результате оценка $\hat a_3$ стремится к нулю, а коэффициенты исходной модели почти не меняются.

При добавлении в выборку одного наблюдения $(k,k)$ с большим $k$:

 - коэффициент при квадратичном члене $\hat a_2(k)$ стремится к нулю — модель «выпрямляется» и ведёт себя почти линейно;

 - коэффициенты $\hat a_0(k)$ и $\hat a_1(k)$ стабилизируются при больших $k$, отражая компромисс между описанием исходных точек и новой, далёкой точки.
