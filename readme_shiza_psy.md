# Домашнее задание: Субъективное семантическое пространство сказочных героев

---

## Цель работы

Целью данной работы является построение **субъективного семантического пространства** восприятия сказочных героев с помощью **ранговых оценок** и **факторного анализа**.  

Метод позволяет выявить скрытые смысловые измерения (факторы), по которым человек воспринимает персонажей — например, «героизм», «интеллектуальность», «доброта» и т. п.  

Построение такого пространства позволяет не только визуализировать структуру субъективных представлений, но и определить, какие черты оказываются наиболее значимыми при формировании эмоционального образа героя.

---

## Задачи исследования

1. Провести **оценку героев** по нескольким биполярным шкалам (например, добрый–злой, смелый–трусливый и т. д.).  
2. Выполнить **ранжирование** оценок от 1 до 10 для каждого героя.  
3. Рассчитать **коэффициенты ранговой корреляции Спирмена и Кендалла** между шкалами, чтобы определить степень их согласованности.  
4. Построить **матрицу корреляций** и на её основе провести **факторный анализ** для выделения смысловых факторов.  
5. Интерпретировать полученные факторы и визуализировать **семантическое пространство героев**.

---

## Исходные данные

В исследовании участвуют **10 сказочных героев**, каждый из которых оценивается по **5 субъективным шкалам** от 1 до 10.  

| № | Герой | Добрый–Злой | Смелый–Трусливый | Активный–Пассивный | Умный–Глупый | Счастливый–Несчастный |
|--:|--|--:|--:|--:|--:|--:|
| 1 | Виолетта Прекрасная | 7 | 7 | 9 | 9 | 7 |
| 2 | Иван-Царевич        | 9 | 8 | 8 | 7 | 9 |
| 3 | Кощей Бессмертный   | 1 | 4 | 3 | 8 | 2 |
| 4 | Колобок             | 7 | 4 | 6 | 5 | 8 |
| 5 | Алиса               | 9 | 3 | 8 | 7 | 10 |
| 6 | Джек Воробей        | 7 | 8 | 9 | 8 | 7 |
| 7 | Дейви Джонс         | 1 | 8 | 3 | 8 | 1 |
| 8 | Алёша Попович       | 9 | 9 | 4 | 4 | 7 |
| 9 | Добрыня Никитич     | 6 | 7 | 5 | 5 | 5 |
|10 | Ариэль              | 10 | 10 | 7 | 6 | 8 |

---

## Формат оценивания

Каждый показатель измеряется **по субъективному восприятию** респондента, где:  
- **1** — минимальное проявление признака (например, «злой», «трусливый», «пассивный»);  
- **10** — максимальное проявление признака (например, «добрый», «смелый», «активный»).

Такой подход позволяет использовать **индивидуальные психологические суждения** для построения многомерного пространства восприятия, в котором каждый герой представлен как точка в пятикоординатном векторном поле.

---

## Этапы анализа

1. Расчёт **коэффициентов ранговой корреляции Спирмена и Кендалла** между шкалами.  
2. Построение **матрицы корреляций**.  
3. Формирование **редуцированной матрицы корреляций** с учётом общностей.  
4. Определение **нагрузок переменных на факторы** методом центроидов.  
5. Интерпретация полученных факторов и визуализация **субъективного семантического пространства** героев.

---

# Шаг 1. Корреляционный анализ (Спирмен и Кендалл) — ПОЛНОЕ ПОЯСНЕНИЕ

В этом разделе подробно описано, **как именно** были получены ранговые корреляции между шкалами  
**Добрый–Злой**, **Смелый–Трусливый**, **Активный–Пассивный**, **Умный–Глупый**, **Счастливый–Несчастный**  
для 10 героев из входных данных.

---

## 1) Подготовка: от исходных баллов к рангам

Мы работаем с **ранговыми корреляциями**, поэтому сначала переводим оценки каждой шкалы в **ранги** $1..10$  
(1 — наименьшее значение, 10 — наибольшее).  
При совпадениях используем правило **средних рангов** (*average ranks*).

Обозначим шкалы:
- $X_1$ — Добрый  
- $X_2$ — Смелый  
- $X_3$ — Активный  
- $X_4$ — Умный  
- $X_5$ — Счастливый  

Полученные (tie-aware) **ранги** для всех 10 героев:

| Герой                | $X_1$ Добрый | $X_2$ Смелый | $X_3$ Активный | $X_4$ Умный | $X_5$ Счастливый |
|:---------------------|-------------:|-------------:|---------------:|------------:|-----------------:|
| Виолетта Прекрасная  | 5.0 | 4.5 | 9.5 | 10.0 | 5.0 |
| Иван-Царевич         | 8.0 | 7.0 | 7.5 | 5.5  | 9.0 |
| Кощей Бессмертный    | 1.5 | 2.5 | 1.5 | 8.0  | 2.0 |
| Колобок              | 5.0 | 2.5 | 5.0 | 2.5  | 7.5 |
| Алиса                | 8.0 | 1.0 | 7.5 | 5.5  | 10.0 |
| Джек Воробей         | 5.0 | 7.0 | 9.5 | 8.0  | 5.0 |
| Дейви Джонс          | 1.5 | 7.0 | 1.5 | 8.0  | 1.0 |
| Алёша Попович        | 8.0 | 9.0 | 3.0 | 1.0  | 5.0 |
| Добрыня Никитич      | 3.0 | 4.5 | 4.0 | 2.5  | 3.0 |
| Ариэль               | 10.0| 10.0| 6.0 | 4.0  | 7.5 |

> Для любого столбца рангов при $n=10$ выполняется $\sum_{i=1}^{10} R_i = 55$,  
> поэтому среднее ранга равно $\overline{R} = 5.5$.

---

## 2) Корреляция Спирмена $\rho_s$

### 2.1. Две эквивалентные формулировки

**(A) Классическая формула (без учёта связей):**

$$
\rho_s = 1 - \frac{6 \sum_{i=1}^{n} d_i^2}{n (n^2 - 1)}, \quad
d_i = R_{X,i} - R_{Y,i}, \quad n = 10.
$$

Эта формула точна при отсутствии связей; при связях её используют как приближение.

---

**(B) Точный вариант — корреляция Пирсона над рангами (учёт связей):**

$$
\rho_s =
\frac{\sum_{i=1}^{n} (R_{X,i} - \overline{R_X})(R_{Y,i} - \overline{R_Y})}
{\sqrt{\sum_{i=1}^{n} (R_{X,i} - \overline{R_X})^2} \;
 \sqrt{\sum_{i=1}^{n} (R_{Y,i} - \overline{R_Y})^2}}.
$$

где $\overline{R_X} = \overline{R_Y} = 5.5$  
(так как ранги распределены от 1 до 10 равномерно).

---

### 2.2. Полный ручной пример (Добрый $X_1$ vs Счастливый $X_5$)

Возьмём два ранговых вектора (из таблицы выше):

- $R_X = (5.0, 8.0, 1.5, 5.0, 8.0, 5.0, 1.5, 8.0, 3.0, 10.0)$  
- $R_Y = (5.0, 9.0, 2.0, 7.5, 10.0, 5.0, 1.0, 5.0, 3.0, 7.5)$

---

#### (A) Приближённо через разности $d_i$

1. Вычисляем $d_i = R_{X,i} - R_{Y,i}$.  
2. Возводим в квадрат: $d_i^2 = (R_{X,i} - R_{Y,i})^2$.  
3. Суммируем все квадраты: $\sum d_i^2 = 27.0$.  
4. Подставляем в формулу:

$$
\rho_s = 1 - \frac{6 \cdot 27}{10 (10^2 - 1)}
= 1 - \frac{162}{990}
= 1 - 0.1636
\approx 0.836.
$$

> Приближённое значение: **$\rho_s \approx 0.836$**

---

#### (B) Точно через Пирсона над рангами (связи учтены)

$$
\rho_s =
\frac{\sum_{i=1}^{10}(R_{X,i}-5.5)(R_{Y,i}-5.5)}
{\sqrt{\sum_{i=1}^{10}(R_{X,i}-5.5)^2} \;
 \sqrt{\sum_{i=1}^{10}(R_{Y,i}-5.5)^2}}
\approx \mathbf{0.829}.
$$

> Для отчёта принимаем **$\rho_s \approx 0.829$**.  
> Разница с (A) объясняется тем, что метод (B) корректно учитывает связи.

---

## 3) Корреляция Кендалла $\tau_b$

Коэффициент Кендалла основан на подсчёте **согласованных** и **несогласованных** пар  
среди всех $\binom{n}{2}$ комбинаций пар объектов.

Для двух шкал $X$ и $Y$ рассматриваются все пары $(i, j)$, где $i < j$:

- пара **согласована**, если знаки $(X_i - X_j)$ и $(Y_i - Y_j)$ совпадают;
- **несогласована**, если знаки противоположны;
- пары со связями по $X$ или $Y$ учитываются отдельно.

Обозначим:
- $C$ — число согласованных пар,  
- $D$ — число несогласованных пар,  
- $T_x$ — количество пар, связанных по $X$,  
- $T_y$ — количество пар, связанных по $Y$.

Формула Кендалла с учётом связей (вариант $\tau_b$):

$$
\tau_b = \frac{C - D}
{\sqrt{(C + D + T_x)(C + D + T_y)}}.
$$

Если связей нет, используется упрощённая форма:

$$
\tau = \frac{C - D}{\binom{n}{2}}.
$$

---

## 4) Итоговые матрицы корреляций (округлены до 3 знаков)

### 4.1. Матрица корреляций Спирмена $\rho_s$

$$
R_{\rho_s} =
\begin{bmatrix}
1.000 & 0.397 & 0.478 & -0.413 & 0.829 \\
0.397 & 1.000 & -0.019 & -0.189 & -0.097 \\
0.478 & -0.019 & 1.000 & 0.305 & 0.609 \\
-0.413 & -0.189 & 0.305 & 1.000 & -0.285 \\
0.829 & -0.097 & 0.609 & -0.285 & 1.000
\end{bmatrix}
$$

---

### 4.2. Матрица корреляций Кендалла $\tau_b$

$$
R_{\tau_b} =
\begin{bmatrix}
1.000 & 0.333 & 0.350 & -0.333 & 0.735 \\
0.333 & 1.000 & -0.024 & -0.150 & -0.049 \\
0.350 & -0.024 & 1.000 & 0.366 & 0.506 \\
-0.333 & -0.150 & 0.366 & 1.000 & -0.148 \\
0.735 & -0.049 & 0.506 & -0.148 & 1.000
\end{bmatrix}
$$

---

### Интерпретация:

- Наиболее сильная положительная связь — между $X_1$ (Добрый) и $X_5$ (Счастливый):  
  $\rho_s \approx 0.829$, $\tau_b \approx 0.735$.  
- $X_3$ (Активный) умеренно коррелирует с $X_1$ и $X_5$.  
- $X_4$ (Умный) слабо связан с остальными.  
- $X_2$ (Смелый) близок к нейтральности.

---

# Шаг 2. Построение редуцированной матрицы корреляций $R'$

Перед переходом к факторному анализу необходимо заменить диагональные единицы в матрице корреляций $R$  
на **начальные приближения общностей** $h_i^{2(0)}$.  
Так мы получаем **редуцированную матрицу корреляций $R'$**, отражающую долю общей дисперсии каждой переменной.

---

## Определение начальных общностей

**Общность** переменной показывает, какая часть её дисперсии объясняется общими факторами.  
На первом шаге (до выделения факторов) общности оцениваются приближённо:

$$
h_i^{2(0)} = \sum_{j \neq i} r_{ij}^2,
$$

где $r_{ij}$ — элемент исходной корреляционной матрицы $R$.

---

## Расчёт начальных общностей по каждой шкале

| Шкала | Расчёт | $h_i^{2(0)}$ |
|:------|:-------|:-------------:|
| **Добрый** | $0.397^2 + 0.478^2 + (-0.413)^2 + 0.829^2 = 0.1576 + 0.2285 + 0.1706 + 0.6872 = 1.2439$ | **1.2439** |
| **Смелый** | $0.397^2 + (-0.019)^2 + (-0.189)^2 + (-0.097)^2 = 0.1576 + 0.0004 + 0.0357 + 0.0094 = 0.2031$ | **0.2031** |
| **Активный** | $0.478^2 + (-0.019)^2 + 0.305^2 + 0.609^2 = 0.2285 + 0.0004 + 0.0930 + 0.3709 = 0.6928$ | **0.6928** |
| **Умный** | $(-0.413)^2 + (-0.189)^2 + 0.305^2 + (-0.285)^2 = 0.1706 + 0.0357 + 0.0930 + 0.0812 = 0.3805$ | **0.3805** |
| **Счастливый** | $0.829^2 + (-0.097)^2 + 0.609^2 + (-0.285)^2 = 0.6872 + 0.0094 + 0.3709 + 0.0812 = 1.1488$ | **1.1488** |

---

## Формирование редуцированной матрицы $R'$

Теперь заменим диагональные элементы исходной матрицы $R$  
на вычисленные общности $h_i^{2(0)}$:

$$
R' =
\begin{bmatrix}
1.2439 & 0.397  & 0.478  & -0.413 & 0.829 \\
0.397  & 0.2031 & -0.019 & -0.189 & -0.097 \\
0.478  & -0.019 & 0.6928 & 0.305  & 0.609 \\
-0.413 & -0.189 & 0.305  & 0.3805 & -0.285 \\
0.829  & -0.097 & 0.609  & -0.285 & 1.1488
\end{bmatrix}
$$

---

## Интерпретация

- В исходной матрице корреляций на диагонали стояли **единицы**, отражающие полную дисперсию каждой переменной.  
- В редуцированной матрице $R'$ на диагонали теперь стоят **оценки общностей**, показывающие долю дисперсии, объясняемую общими факторами.
- На следующем шаге эти значения будут использованы для вычисления **столбцовых сумм $C_{j,1;1}$**  
  и **первых факторных нагрузок $H_{j,1;1}$** в рамках центроидного метода.

---

> Таким образом, матрица $R'$ служит отправной точкой для построения факторной модели —  
> именно из неё начинается процесс выделения скрытых смысловых измерений.

# Шаг 2.3. Вычисление столбцовых сумм $C_{j,1;1}$ и первых факторных нагрузок $H_{j,1;1}$

После построения редуцированной матрицы $R'$ необходимо определить,  
какие переменные вносят наибольший вклад в первый общий фактор.  
Для этого используется **центроидный метод факторного анализа** —  
на первом шаге вычисляются **столбцовые суммы** и **нагрузки первого фактора**.

---

## Формула для столбцовых сумм

По лекции и классическому алгоритму центроидного метода:

$$
C_{j,1;1} = \sum_{i=1}^{p} s_1 \, r'_{ij},
$$

где  
- $r'_{ij}$ — элементы редуцированной матрицы $R'$,  
- $p$ — количество переменных (в нашем случае $p=5$),  
- $s_1$ — общий знак, выбираемый так, чтобы $\lvert C_{j,1;1}\rvert$ были положительными и максимальными (для унификации направлений факторов).

---

## Расчёт для каждого столбца

| Шкала | Расчёт | $C_{j,1;1}$ |
|:------|:--------|-------------:|
| **Добрый** | $1.2439 + 0.397 + 0.478 - 0.413 + 0.829 = 2.5349$ | **2.5349** |
| **Смелый** | $0.397 + 0.2031 - 0.019 - 0.189 - 0.097 = 0.2951$ | **0.2951** |
| **Активный** | $0.478 - 0.019 + 0.6928 + 0.305 + 0.609 = 2.0658$ | **2.0658** |
| **Умный** | $-0.413 - 0.189 + 0.305 + 0.3805 - 0.285 = -0.2015$ | **-0.2015** |
| **Счастливый** | $0.829 - 0.097 + 0.609 - 0.285 + 1.1488 = 2.2048$ | **2.2048** |

---

## Сумма всех столбцовых сумм

$$
\sum C_{j,1;1} = 2.5349 + 0.2951 + 2.0658 - 0.2015 + 2.2048 = 6.8991.
$$

---

## Формула для вычисления нагрузок первого фактора

Нагрузки вычисляются нормировкой столбцовых сумм:

$$
H_{j,1;1} = \frac{C_{j,1;1}}{\sqrt{\sum C_{j,1;1}}}.
$$

В нашем случае  
$\sqrt{\sum C_{j,1;1}} = \sqrt{6.8991} \approx 2.6266.$

---

## Расчёт факторных нагрузок

| Шкала | Расчёт | $H_{j,1;1}$ |
|:------|:--------|-------------:|
| **Добрый** | $2.5349 / 2.6266 = 0.965$ | **0.965** |
| **Смелый** | $0.2951 / 2.6266 = 0.112$ | **0.112** |
| **Активный** | $2.0658 / 2.6266 = 0.787$ | **0.787** |
| **Умный** | $-0.2015 / 2.6266 = -0.077$ | **-0.077** |
| **Счастливый** | $2.2048 / 2.6266 = 0.839$ | **0.839** |

---

## Проверка доли объяснённой дисперсии

Сумма квадратов факторных нагрузок определяет **собственное значение** (eigenvalue) первого фактора:

$$
\lambda_1^{(1)} = \sum_{j=1}^{p} H_{j,1;1}^2
= 0.965^2 + 0.112^2 + 0.787^2 + (-0.077)^2 + 0.839^2
= 0.931 + 0.013 + 0.619 + 0.006 + 0.705 = 2.274.
$$

Доля объяснённой дисперсии:

$$
\frac{\lambda_1^{(1)}}{p} = \frac{2.274}{5} = 0.455 \;(\text{или } 45.5\%).
$$

---

## Интерпретация первого фактора

- Наибольшие положительные нагрузки имеют шкалы:
  - **Добрый (0.97)**  
  - **Счастливый (0.84)**  
  - **Активный (0.79)**  
- «Умный» проявляется слабо отрицательно ($-0.08$),  
  а «Смелый» — почти нейтрален ($0.11$).

**Следовательно**, первый фактор отражает **эмоционально-положительный и деятельный компонент восприятия героя** —  
объединяет представления о доброте, активности и счастье.

---

# Шаг 2.4. Построение матрицы остатков $K_{2;1}$ и выделение второго фактора

После нахождения нагрузок первого фактора $H_{j,1;1}$ необходимо исключить его вклад из редуцированной матрицы $R'$.
Для этого формируется **матрица остатков**:

$$
K_{2;1} = R' - H_{1;1} H_{1;1}^\top
$$

где $H_{1;1}$ — вектор нагрузок первого фактора, представленный в виде столбца:

$$
H_{1;1} =
\begin{bmatrix}
0.965 \\
0.112 \\
0.787 \\
-0.077 \\
0.839
\end{bmatrix}
$$

---

## Внешнее произведение $H_{1;1} H_{1;1}^\top$

Элемент матрицы:

$$
(H_{1;1} H_{1;1}^\top)_{ij} = H_{i,1;1} \cdot H_{j,1;1}
$$

$$
H_{1;1} H_{1;1}^\top =
\begin{bmatrix}
0.931 & 0.108 & 0.759 & -0.074 & 0.809 \\
0.108 & 0.013 & 0.088 & -0.009 & 0.094 \\
0.759 & 0.088 & 0.619 & -0.061 & 0.660 \\
-0.074 & -0.009 & -0.061 & 0.006 & -0.065 \\
0.809 & 0.094 & 0.660 & -0.065 & 0.704
\end{bmatrix}
$$

---

## Вычитание вклада первого фактора

Матрица остатков вычисляется как:

$$
K_{2;1} = R' - (H_{1;1} H_{1;1}^\top)
$$

Подставим значения:

$$
R' =
\begin{bmatrix}
1.2439 & 0.397  & 0.478  & -0.413 & 0.829 \\
0.397  & 0.2031 & -0.019 & -0.189 & -0.097 \\
0.478  & -0.019 & 0.6928 & 0.305  & 0.609 \\
-0.413 & -0.189 & 0.305  & 0.3805 & -0.285 \\
0.829  & -0.097 & 0.609  & -0.285 & 1.1488
\end{bmatrix}
$$

$$
H_{1;1} H_{1;1}^\top =
\begin{bmatrix}
0.931 & 0.108 & 0.759 & -0.074 & 0.809 \\
0.108 & 0.013 & 0.088 & -0.009 & 0.094 \\
0.759 & 0.088 & 0.619 & -0.061 & 0.660 \\
-0.074 & -0.009 & -0.061 & 0.006 & -0.065 \\
0.809 & 0.094 & 0.660 & -0.065 & 0.704
\end{bmatrix}
$$

Вычитаем построчно:

$$
K_{2;1} =
\begin{bmatrix}
0.313 & 0.289 & -0.281 & -0.339 & 0.020 \\
0.289 & 0.190 & -0.107 & -0.180 & -0.191 \\
-0.281 & -0.107 & 0.074 & 0.366 & -0.051 \\
-0.339 & -0.180 & 0.366 & 0.375 & -0.220 \\
0.020 & -0.191 & -0.051 & -0.220 & 0.445
\end{bmatrix}
$$

---

## Столбцовые суммы для второго фактора

По центроидному методу (аналогично первому шагу) вычислим:

$$
S_{j,2;1} = \sum_{i=1}^{p} s_2 \, k_{ij}
$$

где $s_2$ — единый знак, выбранный так, чтобы все суммы имели согласованное направление (здесь $s_2 = +1$).

| Шкала | Расчёт | $S_{j,2;1}$ |
|:------|:--------|-------------:|
| **Добрый** | $0.313 + 0.289 - 0.281 - 0.339 + 0.020 = 0.002$ | **0.002** |
| **Смелый** | $0.289 + 0.190 - 0.107 - 0.180 - 0.191 = 0.001$ | **0.001** |
| **Активный** | $-0.281 - 0.107 + 0.074 + 0.366 - 0.051 = 0.001$ | **0.001** |
| **Умный** | $-0.339 - 0.180 + 0.366 + 0.375 - 0.220 = 0.002$ | **0.002** |
| **Счастливый** | $0.020 - 0.191 - 0.051 - 0.220 + 0.445 = 0.003$ | **0.003** |

Суммарная сумма:

$$
\sum S_{j,2;1} = 0.002 + 0.001 + 0.001 + 0.002 + 0.003 = 0.009
$$

---

## Нагрузки второго фактора

Формула нормировки столбцовых сумм:

$$
H_{j,2;1} = \frac{S_{j,2;1}}{\sqrt{\sum S_{j,2;1}}}
$$

Так как $\sqrt{\sum S_{j,2;1}} = \sqrt{0.009} \approx 0.095$, получаем:

| Шкала | Расчёт | $H_{j,2;1}$ |
|:------|:--------|-------------:|
| **Добрый** | $0.002 / 0.095 = 0.021$ | **0.021** |
| **Смелый** | $0.001 / 0.095 = 0.011$ | **0.011** |
| **Активный** | $0.001 / 0.095 = 0.011$ | **0.011** |
| **Умный** | $0.002 / 0.095 = 0.021$ | **0.021** |
| **Счастливый** | $0.003 / 0.095 = 0.032$ | **0.032** |

---

## Итоговая матрица нагрузок (первое приближение)

$$
H^{(1)} =
\begin{bmatrix}
0.965 & 0.021 \\
0.112 & 0.011 \\
0.787 & 0.011 \\
-0.077 & 0.021 \\
0.839 & 0.032
\end{bmatrix}
$$

---

## Проверка вклада второго фактора

Вычислим собственное значение второго фактора:

$$
\lambda_2^{(1)} = \sum_{j=1}^{5} H_{j,2;1}^2 =
0.021^2 + 0.011^2 + 0.011^2 + 0.021^2 + 0.032^2 = 0.002
$$

Доля объяснённой дисперсии:

$$
\frac{\lambda_2^{(1)}}{p} = \frac{0.002}{5} = 0.0004 \approx 0.04\%
$$

---

## 📘 Вывод

- Второй фактор имеет **очень малые нагрузки** и объясняет менее **0.1%** общей дисперсии.  
- Практически вся структура данных описывается **первым фактором**, отражающим
  **эмоционально-положительный и деятельный компонент восприятия героя**:
  «Добрый», «Счастливый» и «Активный» формируют ядро семантического пространства.

# Шаг 2.5. Уточнение общностей $h_i^2$ и проверка доли объяснённой дисперсии

После выделения двух факторов можно уточнить, какая часть дисперсии каждой переменной объясняется общими факторами.  
Для этого вычисляем **общности** — суммы квадратов нагрузок по всем факторам.

---

## Формулы расчёта

Общность переменной $i$:

$h_i^2 = \sum_{j=1}^{m} H_{i,j}^2$

где $m$ — число факторов (в нашем случае $m = 2$).

Уникальная часть дисперсии (необъяснённая факторами):

$u_i^2 = 1 - h_i^2$

---

## Расчёт общностей и уникальных долей

| Шкала | $H_{i,1}$ | $H_{i,2}$ | $h_i^2 = H_{i,1}^2 + H_{i,2}^2$ | $u_i^2 = 1 - h_i^2$ |
|:------|:----------:|:----------:|:-----------------------------------------:|:-------------------:|
| **Добрый** | 0.965 | 0.021 | $$0.965^2+0.021^2=0.931+0.0004=0.9314$$         | 0.0686 |
| **Смелый** | 0.112 | 0.011 | $0.112^2 + 0.011^2 = 0.0125 + 0.0001 = 0.0126$ | 0.9874 |
| **Активный** | 0.787 | 0.011 | $0.787^2 + 0.011^2 = 0.619 + 0.0001 = 0.6191$ | 0.3809 |
| **Умный** | -0.077 | 0.021 | $(-0.077)^2 + 0.021^2 = 0.0059 + 0.0004 = 0.0063$ | 0.9937 |
| **Счастливый** | 0.839 | 0.032 | $0.839^2 + 0.032^2 = 0.704 + 0.0010 = 0.7050$ | 0.2950 |

---

## Проверка общей объяснённой дисперсии

Сумма общностей по всем переменным:

$\sum h_i^2 = 0.9314 + 0.0126 + 0.6191 + 0.0063 + 0.7050 = 2.2744$

Средняя общность на одну переменную:

$\overline{h^2} = \dfrac{\sum h_i^2}{p} = \dfrac{2.2744}{5} = 0.4549$

То есть два выделенных фактора объясняют около **45.5 %** общей дисперсии.

---

## Интерпретация результатов

| Фактор | Основные нагрузки | Смысловое содержание |
|:-------|:------------------|:----------------------|
| **F₁** | Добрый (0.97), Счастливый (0.84), Активный (0.79) | Эмоционально-положительный и деятельный компонент восприятия |
| **F₂** | Очень малые значения (≤ 0.03) | Вторичный, остаточный фактор — объясняет < 1 % дисперсии |

---

📘 **Вывод:**  
Два выделенных фактора объясняют примерно **45 %** общей изменчивости признаков.  
Первый фактор отражает **оценочно-эмоциональное ядро восприятия** — доброта, активность, счастье.  
Второй фактор практически незначим и фиксирует индивидуальные различия, не влияющие на общий эмоциональный контекст.

# Шаг 2.6. Факторная структура и графическое представление

После выделения двух факторов полезно изобразить переменные (шкалы)  
в координатах первого и второго факторов.  
Это позволяет визуально оценить взаимосвязи и определить смысловые кластеры признаков.

---

## Таблица факторных координат (нагрузок)

| Шкала | $H_{i,1}$ | $H_{i,2}$ | Интерпретация |
|:------|-----------:|-----------:|:--------------------------------|
| **Добрый** | 0.965 | 0.021 | Высокая положительная нагрузка по $F_1$ (эмоционально-положительный полюс) |
| **Смелый** | 0.112 | 0.011 | Почти нейтрален — не входит явно в факторную структуру |
| **Активный** | 0.787 | 0.011 | Близок к «Добрый», отражает деятельность и энергичность |
| **Умный** | −0.077 | 0.021 | Слабо отрицательная связь с $F_1$, независимое измерение |
| **Счастливый** | 0.839 | 0.032 | Один из центральных признаков $F_1$, эмоциональная насыщенность |

---

## Координаты в факторном пространстве

На оси $F_1$ откладываются нагрузки $H_{i,1}$,  
а на оси $F_2$ — $H_{i,2}$.  
Координаты переменных:

$$
\begin{cases}
(\text{Добрый}) = (0.965;\;0.021),\\
(\text{Смелый}) = (0.112;\;0.011),\\
(\text{Активный}) = (0.787;\;0.011),\\
(\text{Умный}) = (-0.077;\;0.021),\\
(\text{Счастливый}) = (0.839;\;0.032)
\end{cases}
$$

---

## Интерпретация осей

**Ось $F_1$ — «Положительно-деятельный фактор»:**

- Высокие значения $F_1$ соответствуют героям, воспринимаемым как **добрые, активные и счастливые**.  
- Фактор отражает **социально-эмоционально положительное восприятие** (оценочно-деятельный компонент).

**Ось $F_2$ — «Остаточный фактор»:**

- Вклад в общую дисперсию минимален (≈ 5 %).  
- Не несёт ярко выраженного смысла и отражает индивидуальные колебания.

---

## Итоговая факторная матрица

$$
H^{(1)} =
\begin{bmatrix}
0.965 & 0.021 \\
0.112 & 0.011 \\
0.787 & 0.011 \\
-0.077 & 0.021 \\
0.839 & 0.032
\end{bmatrix}
$$

$$
\Longrightarrow
\quad
F_1\ \text{– «Эмоционально-положительный фактор»},\qquad
F_2\ \text{– «Остаточный (слабый) фактор»}.
$$

---

📘 **Заключение:**  
Двухфакторная модель субъективного семантического пространства героев показывает,  
что восприятие персонажей определяется в первую очередь **оценочно-эмоциональным фактором**,  
объединяющим доброту, активность и счастье.  
Второй фактор имеет незначительный вклад и фиксирует индивидуальные различия,  
не влияющие на общее эмоциональное восприятие.

